{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Classification\n",
    "\n",
    "This will serve an introduction to `keras`, `tensorflow`, and the basics of neural networks. It will walk through the current eNable framework for the classification of EMG. So let's get to it.\n",
    "\n",
    "## First thing first, what you'll need...\n",
    "0. [Git](https://git-scm.com/)\n",
    "\n",
    "  Git is optional, but it is an excellent tool to use. I will not be covering it in depth here, since the goal for this guide is the neural network, but I would recommend anyone wanting to get into coding to learn how to use this tool. At this [link](https://www.lynda.com/Git-tutorials/Git-Essential-Training/100222-2.html?srchtrk=index%3a1%0alinktypeid%3a2%0aq%3agit%0apage%3a1%0as%3arelevance%0asa%3atrue%0aproducttypeid%3a2) you can find a very useful overview of some of the git commands and practices. Sections 5-7 give the basic overview, but the whole course itself is really good.\n",
    "\n",
    "1. A Python IDE such as [spyder](https://pythonhosted.org/spyder/installation.html) (my personal favorite), [PyCharm](https://www.jetbrains.com/pycharm/), or anything you'd like.\n",
    "\n",
    "2. [Python 3.7](https://www.python.org/downloads/)\n",
    "\n",
    "  With this, the `pip` installer should have been created which can be used to install additional packages using the command line. It can be utilized either through the Windows Command Prompt, or through Git Bash (assuming git was installed as above).\n",
    "\n",
    "3. Run `pip install -r requirements.txt` using the command line in the folder containing this jupyter notebook and the other python files\n",
    "\n",
    "  * This will begin the process of installing all of the required packages:\n",
    "    1. `Tensorflow` for the backend\n",
    "    2. `keras` to make Tensorflow easier to use\n",
    "    3. `SciPy` contains a couple useful mathematical tools\n",
    "    4. `Matplotlib` package to plot the results of the analysis \n",
    "    5. `PyWavelets` takes the raw EMG data and transforms it into wavelet form\n",
    "    6. `Scikit-learn` contains yet more useful mathematical functions\n",
    "    7. `jupyter` which makes this document usable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The framework\n",
    "### Package and function import\n",
    "\n",
    "The required files from both the `keras` package and `nn_analysis_funcs.py` which contains some useful functions for setting up the analysis to keep the `run_analysis_nn.py` clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_analysis_funcs import (import_data, pre_process_data,\n",
    "                               show_confusion_matrix, find_confusion_matrix,\n",
    "                               show_learning, generator)\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from math import floor\n",
    "from functional_NN_EMG import define_NN_architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network architecture\n",
    "\n",
    "The structure of the neural net is defined in `functional_NN_EMG.py` as the function `define_NN_architecture`. The function takes no input and returns the model. It is imported into the program with the following line of code. If you want to change the structure of the neural net into something of your own, editing `define_NN_architecture.py` is the way to go. The neural network data is imported from the `.pkl` file (python version of `.mat` file essentially). The data is then split up into `train_data_x` (the wavelet data), `train_data_x_rms` (root mean square for each of the 16 channels of the EMG), and `train_data_y` (the label assigned to each run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data = import_data('waveletdata.pkl')\n",
    "(train_data_x, train_data_x_rms, train_data_y) = pre_process_data(nn_data)\n",
    "model = define_NN_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up feeding the training data using generators\n",
    "\n",
    "There is one issue with the data in its raw format: it's too big. The data itself takes up more than 1 GB in its raw form. This leads an issue with the memory when the network is trying to train the millions of parameters needed for the network. Instead then of using the raw data, we use a generator instead (see `generator` in `nn_analysis_funcs.py` for how this functions is built and [this link](https://wiki.python.org/moin/Generators) for what a generator is in python). This prevents the entire data from being loaded into memory and is instead fed piecemeal into the fitting process.\n",
    "\n",
    "The neural network itself trains of 70% of the data, seeing how the updated weights affect the classification of the entire data set. The neural network, once it runs through the entire training set, will test those weights on a separate data set, the validation set. This set makes sure that the learning of the dataset is actually generalizable to a separate data set. Finally, the testing set verifies how the fully trained model deals with data it has never seen before.\n",
    "\n",
    "Finally, the parameter batch_size (in `run_NN_analysis.py` it is passed as a parameter into the `main` function), specifies how much of the data set to load into at once. This is highly dependent on your system. You can play around with this value to whatever seems to result in the fastest training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_step_length = int(0.7*len(train_data_y))\n",
    "val_length = int(0.2*len(train_data_y))\n",
    "test_length = int(0.1*len(train_data_y))\n",
    "\n",
    "train_gen = generator(train_data_x, train_data_x_rms,\n",
    "                      train_data_y, batch_size,\n",
    "                      0, train_step_length)\n",
    "\n",
    "val_gen = generator(train_data_x, train_data_x_rms,\n",
    "                    train_data_y, batch_size,\n",
    "                    train_step_length + 1, train_step_length + val_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model training\n",
    "\n",
    "First, the optimizer is defined that the model will use. In this case, I've chosen the Adadelta algorithm. There are many other built-in to keras. See [here](https://keras.io/optimizers/) for more details.\n",
    "\n",
    "Since the model uses categorical labels for data, and each run is only assigned one label, the loss function is `cateogrical_crossentropy`, where the metric we are interested in is the accuracy of the run.\n",
    "\n",
    "The additional checkpoint function is merely a method to save the best model over all the tested epochs (iterations over the full data set).\n",
    "\n",
    "The `train_step_num` and the `val_step_num` have to be calculated for the model fit, and are automatically done by the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adadelta_optim = Adadelta(lr=1, rho=0.95, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adadelta_optim,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint_func = ModelCheckpoint('best_model.hdf5',\n",
    "                                  monitor='val_acc',\n",
    "                                  save_best_only=True,\n",
    "                                  mode='max')\n",
    "\n",
    "train_step_num = floor(train_step_length/batch_size)\n",
    "val_step_num = floor(val_length/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pièce de résistance... running the fit\n",
    "\n",
    "These lines of code actually set up the fit of the neural network. The generators are imported, the number of steps needed to perform for each epoch, the number of epochs to run the model (35-50 is usually the amount it takes before the validation accuracy plateaus), the validation generator, the number of validation steps, and then adding in the callback function into the model fit.\n",
    "\n",
    "**This step will take quite a while. 8 hours running on a CPU is quite realistic and it could take even longer depending on model complexity. This is why GPU computation has been incredibly useful for neural networks (and outside the scope of this guide).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_gen,\n",
    "                           steps_per_epoch=train_step_num,\n",
    "                           epochs=35,\n",
    "                           validation_data=val_gen,\n",
    "                           validation_steps=val_step_num,\n",
    "                           callbacks=[checkpoint_func])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing the model\n",
    "\n",
    "A confusion matrix is a matrix showing the model labeling versus the true labeling of the data. Note: the matrix visualization does not render well in the cells below, but works well in a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = find_confusion_matrix(model,\n",
    "                                    [train_data_x[-test_length:],\n",
    "                                    train_data_x_rms[-test_length:]],\n",
    "                                    train_data_y[-test_length:])\n",
    "show_confusion_matrix(conf_matrix)\n",
    "show_learning(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
